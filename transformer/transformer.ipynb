{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer架构\n",
    "<div align=center><img decoding=\"async\" src=img/transformer.webp width=\"60%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建config类，用于保存超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 6\n",
    "\n",
    "        self.d_model = 20\n",
    "        self.n_heads = 2\n",
    "\n",
    "        assert self.d_model % self.n_heads == 0\n",
    "        self.dim_k = self.d_model % self.n_heads\n",
    "        self.dim_v = self.d_model % self.n_heads\n",
    "\n",
    "        self.padding_size = 30\n",
    "        self.UNK = 5\n",
    "        self.PAD = 4\n",
    "\n",
    "        self.N = 6\n",
    "        self.p = 0.1\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding 进行元素编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(Embedding, self).__init__()\n",
    "        # 一个普通的 embedding层，我们可以通过设置padding_idx=config.PAD 来实现论文中的 padding_mask\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, config.d_model, padding_idx=config.PAD\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 根据每个句子的长度，进行padding，短补长截\n",
    "        for i in range(len(x)):\n",
    "            if len(x[i]) < config.padding_size:\n",
    "                x[i].extend(\n",
    "                    [config.UNK] * (config.padding_size - len(x[i]))\n",
    "                )  # 注意 UNK是你词表中用来表示oov的token索引，这里进行了简化，直接假设为6\n",
    "            else:\n",
    "                x[i] = x[i][: config.padding_size]\n",
    "        x = self.embedding(torch.tensor(x))  # batch_size * seq_len * d_model\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "position embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Positional_Encoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Positional_Encoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, seq_len, embedding_dim):\n",
    "        positional_encoding = np.zeros((seq_len, embedding_dim))\n",
    "        for pos in range(positional_encoding.shape[0]):\n",
    "            for i in range(positional_encoding.shape[1]):\n",
    "                positional_encoding[pos][i] = (\n",
    "                    math.sin(pos / (10000 ** (2 * i / self.d_model)))\n",
    "                    if i % 2 == 0\n",
    "                    else math.cos(pos / (10000 ** (2 * i / self.d_model)))\n",
    "                )\n",
    "        return torch.from_numpy(positional_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, dim_k, dim_v, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.dim_k = dim_k\n",
    "        self.dim_v = dim_v\n",
    "        self.d_model = d_model\n",
    "        self.q = nn.Linear(d_model, dim_k)\n",
    "        self.k = nn.Linear(d_model, dim_k)\n",
    "        self.v = nn.Linear(d_model, dim_v)\n",
    "        self.o = nn.Linear(dim_v, d_model)\n",
    "        self.__normal_factor = 1 / math.sqrt(d_model)\n",
    "\n",
    "    def generate_mask(self, dim):\n",
    "        mask = torch.from_numpy(np.triu(np.ones((dim, dim)), k=1).astype(\"bool\"))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x, y, require_mask=False):\n",
    "        Q = self.q(x)\n",
    "        K = self.k(x)\n",
    "        V = self.v(y)\n",
    "\n",
    "        Q_ = Q.view(Q.shape[0], Q.shape[1], self.n_head, self.dim_k // self.n_head)\n",
    "        K_ = K.view(K.shape[0], K.shape[1], self.n_head, self.dim_k // self.n_head)\n",
    "        V_ = V.view(V.shape[0], V.shape[1], self.n_head, self.dim_v // self.n_head)\n",
    "\n",
    "        AttenMatrix = torch.matmul(Q_, K_.permute(0, 1, 3, 2)) * self.__normal_factor\n",
    "\n",
    "        if require_mask:\n",
    "            mask = self.generate_mask(AttenMatrix.shape[1])\n",
    "            AttenMatrix.masked_fill_(mask, -float(\"inf\"))\n",
    "\n",
    "        output = torch.matmul(AttenMatrix, V_).reshape(Q.shape[0], Q.shape[1], -1)\n",
    "\n",
    "        output = self.o(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feed_forward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Feed_forward, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add&Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add_Normal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Add_Normal, self).__init__()\n",
    "        self.dropout = nn.Dropout(config.p)\n",
    "\n",
    "    def forward(self, x, sub_layer, **kwargs):\n",
    "        sub_output = sub_layer(**kwargs)\n",
    "        x = self.dropout(x + sub_output)\n",
    "        layer_norm = nn.LayerNorm(x.size()[1:])\n",
    "        x = layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.positional_encoding = Positional_Encoding(config.d_model)\n",
    "        self.multi_head_attention = MultiHeadAttention(\n",
    "            config.d_model, config.dim_k, config.dim_v, config.n_heads\n",
    "        )\n",
    "        self.feed_forward = Feed_forward(config.d_model)\n",
    "        self.add_normal = Add_Normal()\n",
    "\n",
    "    def forward(self, x):\n",
    "        positional_encoding = self.positional_encoding(x.shape[1], x.shape[2])\n",
    "        x = x + positional_encoding\n",
    "        x = self.add_normal(x, self.multi_head_attention, x=x, y=x)\n",
    "        x = self.add_normal(x, self.feed_forward)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
